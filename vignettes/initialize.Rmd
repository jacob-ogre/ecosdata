---
title: "Initialize ECOS archive"
author: "Jacob Malcom, Defenders of Wildlife"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: yeti
---

```{r setup, include=FALSE}
library(digest)
library(dplyr)
library(ecosdata)
library(ecosscraper)
library(knitr)
library(parallel)
library(purrr)

NCORE <- detectCores() - 1
BASED <- "~/Work/Data/ECOS"
```

`r Sys.time()`

# Get Base Data

## TECP_table

Before initializing the run across all T & E species, we need to get up-to-date
data from the main table of ECOS:

```{r TECP}
TECP_init <- get_TECP_baseline()
TECP_table <- TECP_init$TECP_table
TECP_summary <- TECP_init$TECP_summary
kable(head(TECP_table))
```

At this initial scrape, it is interesting to see one Hawaiian species' common
name, which features backticks, gets turned to "code" (e.g., _Abutilon menziesii_).
We will keep the summary df of the TECP_table for joining with the per-species 
page scrape summary data, to be saved later.

# Get Species' Pages

## Downloading

Now we can get every species' ECOS page and save them all locally, which will
facilitate all sorts of processing:

```{r scrape_species_pages}
######################################################################
# The initial scraping was done 07 Dec 2016 ca. 2am, but other R code chunks
# run later in the day to 
urls <- TECP_table$Species_Page[1:5]
dirs <- paste0(BASED, "/species/", TECP_table$Species_Code[1:5])
res <- lapply(dirs, function(x) if(!dir.exists(x)) dir.create(x, recursive = TRUE))
fils <-  file.path(dirs, paste0(TECP_table$Species_Code[1:5], 
                                "_", Sys.Date(), ".html"))
results <- mcmapply(download_species_page, urls, fils, SIMPLIFY = FALSE,
                    mc.cores = NCORE, mc.preschedule = FALSE)
results <- bind_rows(results)
ECOS_dl_08Dec2016 <- results
save(ECOS_dl_08Dec2016, 
     file = paste0(BASED, "/rda/ECOS_dl_08Dec2016.rda"))
```

Oh, the possibilities! For example, we could create a parallel ECOS that 
includes more information in a better layout than ECOS provides...hmmmm.

# Page Processing

Now that we have local copies, it's time to get out the information we need.

## Page hashes

First, we need the MD5 hash of the page content, which will be the first step in
determining if pages have changed since the last scrape.

```{r md5_hash}
files <- ECOS_dl_08Dec2016$dest
md5s <- mclapply(files,
                 species_page_md5,
                 mc.cores = NCORE,
                 mc.preschedule = FALSE)
ECOS_dl_08Dec2016$MD5 <- unlist(md5s)
kable(head(ECOS_dl_08Dec2016, 10))
```

## Page links

We will want all of the links (URLs) on each species' page, not only to fetch 
that information but also to check whether any changes detected from the hashing
are 'substantive' changes.

```{r get_links}
sp_links <- mclapply(files, 
                     get_species_links,
                     mc.cores = NCORE,
                     mc.preschedule = FALSE)
ECOS_species_links <- bind_rows(sp_links)
kable(head(ECOS_species_links, 10))
```

That gives us `r length(ECOS_species_links[[1]])` URLs that may be of use. Just 
out of curiosity, how many are links to images? (Note that this does not include
the PNGs of species' ranges.)

```{r img_links}
img <- filter(ECOS_species_links,
              grepl(ECOS_species_links$link,
                    pattern = "jpg$|JPG$|gif$|png$"))
dim(img)
kable(head(img))
```

## Page tables

The tables on each species' page contain useful information, so we will extract
all the tables.

```{r get_tables}
tabs <- mclapply(files,
                 get_species_tables,
                 mc.cores = NCORE,
                 mc.preschedule = FALSE)
tab_names <- map(1:length(tabs), function(x) names(tabs[[x]])) %>%
               unlist() %>% unique()
names(tabs) <- c(as.character(seq(1, length(tabs))))
SP_TABS <- bind_tables(tabs, "SP_TAB")
FR_TABS <- bind_tables(tabs, "FR_TAB")
REC_TABS <- bind_tables(tabs, "REC_TAB")
DOC_TABS <- bind_tables(tabs, "DOC_TAB")
REV_TABS <- bind_tables(tabs, "REV_TAB")
CH_TABS <- bind_tables(tabs, "CH_TAB")
HCP_TABS <- bind_tables(tabs, "HCP Plan Summaries")
SHA_TABS <- bind_tables(tabs, "SHA Plan Summaries")
CCA_TABS <- bind_tables(tabs, "CCA Plan Summaries")
CCAA_TABS <- bind_tables(tabs, "CCAA Plan Summaries")
kable(head(SP_TABS, 10))
```

# Save Data

Last, for now, we will combine the data sets as appropriate and save them as
`.rda`s:

```{r save_data}
# save(ECOS_dl_data, file = "/datadrive/data/ECOS/rda/ECOS_dl_data.rda")
# save(TECP_domestic, file = "/datadrive/data/ECOS/rda/TECP_domestic.rda")
save(TECP_summary, file = "/datadrive/data/ECOS/rda/TECP_summary_07Dec2016.rda")
```

`r Sys.time()`
